{
  
    
        "post0": {
            "title": "The MNIST Experiment",
            "content": "Welcome! . Hello and welcome to another of my adventures! Today we will be building a neural network using the MNIST Digits Data Set for digit detection. . We will have a chance to ”decompose” a simple neural network, see how it looks from the inside, we will create our loss function, accuracy metric and see how a network learns, all within this exercise. . I’m so excited to share this with you and I want to say that doing this small project was and still is one of my favorite projects I have ever done. . Now before we start I should say, I had some issues with math, I forgot a lot of calculus and had to go back and review some concepts, including some calculus (derivatives), Deep Learning parameters and hyperparameters and some FastAI, pytorch, python functions that made the exercise easier to practice. . Finally I would like to say this exercise is based on FastAI Course Lesson 4. in case you want to review some further details regarding this and other exercises. . Data Structure Review . First things first, let&#39;s unpack the data and review it. FastAI already has a way for us to download the MNIST data set in full. We will download it, unpack it, and see its contents. . Some key questions we want answers for are: . How is the data structured? | Does it have a validation set? | How is the data labeled? | . path = untar_data(URLs.MNIST) . Path.BASE_PATH = path path.ls() . (#2) [Path(&#39;training&#39;),Path(&#39;testing&#39;)] . Ok so we can see the data is divided into 2 separate data sets, testing (validation) and training sets and each of them contains a folder that identifies the digit of the images it contains. We should be able to examine the contents of one of the folders and render one of the images. . (path/&#39;training&#39;).ls().sorted(), (path/&#39;testing&#39;).ls().sorted() . ((#10) [Path(&#39;training/0&#39;),Path(&#39;training/1&#39;),Path(&#39;training/2&#39;),Path(&#39;training/3&#39;),Path(&#39;training/4&#39;),Path(&#39;training/5&#39;),Path(&#39;training/6&#39;),Path(&#39;training/7&#39;),Path(&#39;training/8&#39;),Path(&#39;training/9&#39;)], (#10) [Path(&#39;testing/0&#39;),Path(&#39;testing/1&#39;),Path(&#39;testing/2&#39;),Path(&#39;testing/3&#39;),Path(&#39;testing/4&#39;),Path(&#39;testing/5&#39;),Path(&#39;testing/6&#39;),Path(&#39;testing/7&#39;),Path(&#39;testing/8&#39;),Path(&#39;testing/9&#39;)]) . Image.open((path/&#39;training&#39;/&#39;8&#39;).ls().sorted()[999]) . It looks like a very simple image that contains the digit number in black and white. As we saw from the details above I think we have a way to answer our questions and that will help us generate our dataset: . How is the data structured? . We have 2 folders 1 for the training data and one for testing data. . Does it have a validation set? . Yes . How is the data labeled? . Within each of the folders (testing and training) we have child folders that represent the digits and finally within those we have our data . Data Set Definition . Using the answers from the questions above we need to come up with a small script that helps us build our data set. . Here are the criteria for building our script: . Grab the images within our training and testing folders. | Traverse each folder structure in order to grab all the digit folders within. | Grab the images within each digit folder and store them in a way we can label and use them for training and validation. | Use the folder names to generate our labels for each of the images | # This grid will be used for tracking our results from processing the images training_results = create_grid(len((path/&#39;training&#39;).ls().sorted())) # First lets grab all of our images and store the corresponding paths in # a list so that we can read them training_folders = (path/&#39;training&#39;).ls().sorted() training_paths = [] # This tensor will contain the label for each of the images in the data set train_y = tensor([]) # Index for our grid index = 0 # We need to loop over the different paths and then grab the paths to # all of our images sorted so that we can load the image into a # tensor we can use to pass it to model for digits in training_folders: # Grab the digit label in the path of the folder label = re.findall(&quot; d&quot;,str(digits)) # Create a label tensor with equal to the number of images for the digit label_tensor = tensor([float(label[0])]*len(digits.ls())) train_y = torch.cat((train_y, label_tensor),0) # Grab all the paths to our images training_paths += digits.ls().sorted() # Display our results process_row(training_results, index, label[0],len(digits.ls())) index+=1 train_y = train_y.type(torch.LongTensor) # We can check the our array of paths tensor shape should be # (total number images in all folders) training_results . . Now that we have our training_paths we can use it to open the images, map the pixel values to a tensor and finally divide them by 255 since we want values that fit between 0 and 1 for the pixel images. . # Final we should be able to open the image and store it as a tensor train_data = torch.stack([tensor(Image.open(o)) for o in training_paths.sorted()]).float()/255 # We can check the our tensor shape should be # (total number images, with a 28 by 28 size) Rank 3 tensor train_data.shape, train_y.shape . (torch.Size([60000, 28, 28]), torch.Size([60000])) . Validation Data Set . Once we have processed the training data set we can perform the same routine to the validation data set so that we can: . Collect all the validation set images. | Generate the label data. | Process the images in a way our model can read it. | validation_results = create_grid(len((path/&#39;training&#39;).ls().sorted())) validation_labels = (path/&#39;testing&#39;).ls().sorted() validation_paths = [] index = 0 valid_y = tensor([]) for digits in validation_labels: label = re.findall(&quot; d&quot;,str(digits)) label_tensor = tensor([float(label[0])]*len(digits.ls())) valid_y = torch.cat((valid_y, label_tensor),0) validation_paths += digits.ls().sorted() process_row(validation_results, index, label[0],len(digits.ls())) index+=1 valid_y = valid_y.type(torch.LongTensor) validation_results . . We can now perform the same functionality as in the training set: . Open the images as a tensor. | Convert the numbers to be between 0 and 1. | . valid_data = torch.stack([tensor(Image.open(o)) for o in validation_paths]).float()/255 valid_data.shape, valid_y.shape . (torch.Size([10000, 28, 28]), torch.Size([10000])) . Data Review . When dealing with images, something that really helped me was to visualize what was going to happen and experiment a little with the data. Jeremy did something similar in the course where he opened the image so we can see the underlying data and try to understand and figure out how the machine would process it. . So in the next cell we will be grabbing one of the images in the training data, open it with pandas and render the contents using a data frame grid. . a_digit = train_data[51000,:,6:24] df = pd.DataFrame(a_digit) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . . After seeing how the data is composed, we can see that: . Each image has numbers from 0 to 1 that represent each of the pixels in the image. | We can use those pixel numbers and train a neural network to be able to identify the different digits. | . In order to use this data we will need to perform 1 more transformation... . Tuples and Data Sets . We need to transform the training images like the one above into a vector by stacking all the pixels into 1 big column, and, create a tuple with the vectorized image and the corresponding label. We need to pass a list of tuples to the data loaders class which FastAI uses to create the training batches used when training a model. . # The 1st value is the index and 2nd will be the image decomposed into a vector train_x = train_data.view(-1, 28*28) valid_x = valid_data.view(-1, 28*28) train_x.shape, valid_x.shape . (torch.Size([60000, 784]), torch.Size([10000, 784])) . train_dset = list(zip(train_x,train_y)) valid_dset = list(zip(valid_x,valid_y)) . Now the same image above has been transformed into a vector and is followed by the corresponding label at the end of the tuple. (The numbers below represent the image above) . train_dset[51000] . (tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1255, 0.5804, 0.9922, 0.9922, 0.3137, 0.0000, 0.0000, 0.2157, 0.5804, 0.4863, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7451, 0.9882, 0.9882, 0.9882, 0.9529, 0.1882, 0.2039, 0.9098, 0.9882, 0.8824, 0.0824, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0353, 0.5686, 0.9686, 0.9882, 0.9882, 0.9882, 0.8745, 0.1255, 0.4980, 0.9882, 0.9882, 0.9804, 0.2275, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627, 0.7294, 0.9922, 0.9882, 0.9686, 0.7412, 0.5686, 0.1373, 0.0000, 0.7725, 0.9882, 0.9882, 0.7255, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2510, 0.9882, 0.9922, 0.9882, 0.3882, 0.0000, 0.0000, 0.0000, 0.4549, 0.9765, 0.9882, 0.8157, 0.0706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2510, 0.9922, 0.9961, 0.9373, 0.0667, 0.0000, 0.0000, 0.0980, 0.8902, 0.9922, 0.8471, 0.0706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2510, 0.9882, 0.9922, 0.9059, 0.2000, 0.0000, 0.0000, 0.6157, 0.9882, 0.9882, 0.4314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2510, 0.9882, 0.9922, 0.9882, 0.7412, 0.2000, 0.2431, 0.9922, 0.9882, 0.7059, 0.0275, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0824, 0.6039, 0.9922, 0.9882, 0.9882, 0.9059, 0.9373, 0.9922, 0.9059, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.8196, 0.9882, 0.9882, 0.9882, 0.9882, 0.9922, 0.6588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.4157, 0.8784, 0.9922, 0.9922, 1.0000, 0.9922, 0.3922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4980, 0.9882, 0.9882, 0.9922, 0.9882, 0.9686, 0.3882, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8784, 0.9882, 0.9569, 0.7490, 0.9882, 0.9882, 0.8667, 0.0824, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2235, 0.9647, 0.9882, 0.5765, 0.0549, 0.8275, 0.9882, 0.9882, 0.4431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6784, 0.9882, 0.9882, 0.5765, 0.0000, 0.7451, 0.9882, 0.9882, 0.7490, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7490, 0.9922, 0.9922, 0.1647, 0.2392, 0.8902, 0.9922, 0.9922, 0.3373, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9882, 0.9882, 0.7843, 0.9922, 0.9882, 0.9882, 0.8039, 0.0431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3059, 0.9843, 0.9882, 0.9882, 0.9922, 0.9882, 0.7412, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8078, 0.9882, 0.9882, 0.9922, 0.8039, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2902, 0.8510, 0.9882, 0.5765, 0.1451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), tensor(8)) . Data Loaders . The DataLoader class in FastAI is a class that will help us generate a mini batch of training and validation data that will be processed by the model in each epoch. . The Data Loader class can be used even if we are not using a fully developed cnn_learner from FastAI, like, when we are customizing our learner or defining a model from scratch. . Something I thought was very cool was that this decoupling of the architecture helps when people want to build a customized model; we have libraries, classes or pieces of code we can reuse; with the DataLoader class don&#39;t have to reinvent the wheel to create a class or method that help us load our data to our model . train_dl = DataLoader(train_dset, batch_size=256, shuffle=True) valid_dl = DataLoader(valid_dset, batch_size=256, shuffle=True) # Next we will crate our neural network which will use the DataLoaders class that # holds our training and validation dataset dls = DataLoaders(train_dl, valid_dl) . Accuracy and Loss . These are 2 concepts that as Jeremy explained in the course can often be confused because both are a representation of our model performance. . Accuracy usually represents the average of correct predictions in a model. | Loss represents the rate of change of how much should we &quot;update&quot; (step) our weights to improve our predictions taking into account, the current learning rate, input and parameters for our model. Think of it as the penalty for making a bad prediction. | . The loss is used in Gradient Descent by the machine to understand with that penalty or “punishment” if the parameters should be updated or not depending on the result of the prediction; taking into account that the loss function will penalize/punish heavily on worse predictions or lightly on better predictions. . On the other hand accuracy is used by us, humans, understand the performance of our model so that we can validate and make decisions on whether we should be altering the hyperparameters used by the learning process (batch size, number of epochs) to improve the performance of our model. . # This accuracy function will calculate the accuracy comparing the max values of the # inputs using argmax to get the max values of the passed inputs and then checking it # agaist the targets and finally calculate the mean def batch_accuracy(xb, yb): xb = np.argmax(xb, 1) correct = (xb==yb) return correct.float().mean() . def mnist_loss(predictions, targets): # When using softmax we need to ensure the number of activations in our # last layer matches the number of classes for our model predictions in this case should be 10 predictions = torch.log_softmax(predictions, 1) return F.nll_loss(predictions, targets) . The Neural Network . Finally we have reached the moment to create our neural network. You will see that the definition of a network is actually not as complex we might think and you might be surprised (I was really surprised!!) that its composed of 3 functions, and how much it can do with only those 3 layers. Here is the definition of our very simple neural net, the function is actually small, and it has 3 different components: . simple_net(xb){ res = xb@w1 + b1 res = res.max(tensor(0.0)) res = res@w2 + b2 return res } . Layer that performs a matrix multiplication using the values of the image we are trying to predict times the layer 1 weights then adds it up with a bias. | (Activation) Rectifier linear unity which will basically change any negative result from the previous layer into a 0. | Final layer which will perform a matrix multiplication with the result of the previous operation times the weights of layer 2 and adds it up again to bias of layer 2. | simple_net = nn.Sequential( nn.Linear(28*28,50), nn.ReLU(), nn.Linear(50,10) ) . Learner Module . FastAI has a learner module that enables us to use our own model, optimizer, loss and metrics. The module would help us with: . Executing the training epochs. | Call our optimizer to update our parameters. | Using the data loaders pass the mini batches to the training. | Call our metric functions. | Give us an output of our training. | . I found this module very helpful, similarly to the DataLoaders module, the learner module decouples the API and for more advanced people enables them to customize every detail on the model and how they want to optimize the weights, metrics they want to use and more. . learn = Learner(dls, simple_net, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) . learn.fit(10, 0.1) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.090545 | 0.108007 | 0.966200 | 00:01 | . 1 | 0.088196 | 0.104899 | 0.966700 | 00:01 | . 2 | 0.090239 | 0.108342 | 0.966000 | 00:01 | . 3 | 0.087355 | 0.104934 | 0.967000 | 00:02 | . 4 | 0.080858 | 0.102985 | 0.968600 | 00:02 | . 5 | 0.081753 | 0.101288 | 0.968400 | 00:02 | . 6 | 0.079057 | 0.100700 | 0.968800 | 00:02 | . 7 | 0.078785 | 0.098953 | 0.969800 | 00:02 | . 8 | 0.073720 | 0.101644 | 0.968800 | 00:02 | . 9 | 0.075326 | 0.097657 | 0.970700 | 00:01 | . Visualizing Layer Features . In this final section we will use the parameters stored in our model and generate a small image. I loved this part of the exercise since it&#39;s where we can &quot;see&quot; how our network defines the features on the layer/s. . The nn.Sequential function used in our model stores the different functions in a list. We can use the index to grab the layer we want to use for the rendering (which is layer 0). . simple_net[0].parameters() . You will see that the line above contains a tensor with 50 features each containing a vector of size 784. We will use what we did in the past to transform the vector into a 28x28 tensor and finally use the show_images method to render that tensor as an image. . w, b = simple_net[0].parameters() w.shape . torch.Size([50, 784]) . # We can also render a feature using pandas and do it on a grey scale df2 = pd.DataFrame(w[1].view(28,28)*-1) df2.style.set_properties(**{&#39;font-size&#39;:&#39;1pt&#39;}).background_gradient(&#39;Greens&#39;) . . layer_index = 0 layers_to_render = [0] # We will use these arrays to store the images and the titles of each of them images = [] titles = [] # Finally let&#39;s loop the layers of the nn.Sequetial for layer in simple_net: if layer_index in layers_to_render: index = 0; w, b = layer.parameters() # Finally we can change the tensor to a 28 by 28 to render the layers for feature in w: images.append(feature.view(28,28)) titles.append(str(f&#39;Feature: {index}&#39;)) index+=1 layer_index+=1 show_images(images, titles=titles, nrows=8, ncols=5, imsize=4) . Summary . Thank you so much for reaching the end, I really appreciate you taking the time to reading and reviewing my post and joining me in my journey through Deep Learning. . Today with this exercise we learned about the different modules FastAI exposes and how we can use them to build our custom model. . We created our accuracy and loss function and also loaded the data into data loaders that helped us when we were training our model. . Finally we explored the parameters resulting of the training and a potential path to render the parameters of a network to see the different features a model might use, and how they look . I hope this was helpful and useful to you. If you were following the exercise that you found it interesting and fun to build. . Thanks again and see you on my next adventure. . Resources and References . Jeremy Howard, Rachel Thomas and Sylvain Gugger, FastAI Course, https://course.fast.ai/videos/?lesson=4 | Grant Sanderson, The Essence of Calculus, 3blue1brown, https://youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr | Tuples https://docs.python.org/3/tutorial/datastructures.html#tuples-and-sequences | Jason Brownlee, What is the Difference Between a Parameter and a Hyperparameter?, Machine Learning Mastery, Available from https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/ | .",
            "url": "https://victor-vargas2009.github.io/gamescode/fastai/vision/2021/05/08/The-MNIST-Experiment.html",
            "relUrl": "/fastai/vision/2021/05/08/The-MNIST-Experiment.html",
            "date": " • May 8, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "RE Biological Detector Using FastAI Vision",
            "content": "Resident Evil Biological Detector . It was April 12, 2021, 8PM as I started my first playthrough of Resident Evil 3 remake, playing as Jill Valentine S.T.A.R.S. of the Racoon City Police Department you start to realize how bad the situation is right from the beginning of the game you get to see everything that is going on in the city; zombies everywhere people running scared and all of a sudden the wall explodes and Jill has one more thing to to worry about. . NEMESIS. . That big ugly thing that is all of a sudden chasing you everywhere you go and the first question Jill asks was: . &quot;What the hell is that thing?&quot; . And I was well yeah what is that thing there is a big ugly powerful monster chasing me...It would be very helpful I knew what it was maybe just to know what can I do to protect myself and that&#39;s how I thought well maybe its time we give our fellow S.T.A.R.S. members some help to identify those creatures that way at least they know what they are fighting against. . Background . I recently restarted the FastAI Course V4 with Jeremy Howard, Rachel Thomas and Sylvain Gugger, I originally took the live version in 2020 but due to the pandemic it was all done remote. I finished the whole course with them and it was great but there was a lot of information to process for my little brain, so I decided to stop and restart later once they published the new version. . After a while I decided to get back on DL and restart the whole course and in the course it was recommended us to create a blog, which is the main reason for me to pursue this as well. Needless to say that I will blogging about the entire experience and exercises of the FastAI course. . Finally, I want to take this time to thank the course instructors Jeremy, Rachel and Sylvain for inspiring us (your students) to blog, try, learn something new and also to make DL and ML more accessible to a lot of people and easy to understand, Thanks You. . And so without further ado lets get started... . Data Gathering . Since I don&#39;t have a data set of Resident Evil creatures (although Umbrella for sure has one), I&#39;m going to be searching images that I can use to identify all these creatures and use those for training and validation set. The FastBook included a couple functions that we can use to accomplish this by using third party search APIs. FastBook has 2 API functions, one for bing and one for duck duck go. We can use either of those to build our data set for our prototype. . # we are defining a path so that they can be saved locally monster_labels = &#39;t-type nemesis&#39;,&#39;licker&#39;,&#39;hunter&#39;,&#39;zombie&#39;, &#39;cerberus&#39; path = Path(&#39;storage/resident_evil/monsters&#39;) . The next lines of code will perform a search of the different monsters and download each under the folder names that we will be using for labeling later. Labeling data is very important and FastAI has several APIs that you can use to pass the labels to the data set, this is just one of the many ways we can do it. . if not path.exists(): path.mkdir() for o in monster_labels: dest = (path/o) if not dest.exists(): dest.mkdir(exist_ok=True) # This is the duck duck go image search but FastBook included other to use Bing results = search_images_ddg(f&#39;Resident Evil {o}&#39;) download_images(dest, urls=results) . Data preprocessing . As part of the download we will start by cleaning up and validating out images. The get_image_files() function is used for obtaining the image path and the we will execute the verify_images() function after to make sure we remove all the images that can&#39;t be opened. . This is a pretty useful function since it will allow us to clear our already issues with some of our data. . fns = get_image_files(path) # Returns a list of file names with paths failed = verify_images(fns) # Function will find images that cant be opened failed . failed.map(Path.unlink); . Datablock Definition . The DataBlock API I understand it as a major link in all the FastAI libraries. This API will be in charge of a lot of things: . Defining our data set type (image vs tabular) | How do access or read the data | Where/How do we use our labels | How should FastAI split the data used for validation and training | Define transformers applied to the items (data) | . The next lines of code we will be using the data block API to generate a new type of block that will use a single category and use images for the data. We will obtain items using the get_image_files function and split the validation data set to a .3 percent, the label data will be obtained via the parent folder and finally we will define a transform that will randomly crop and resize images to 128 . monsters = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.3, seed=42), get_y=parent_label, item_tfms=RandomResizedCrop(128)) . About Data Augmentation . As part of training our model, we explored only a little of data augmentation in the course. The concept is that a deep learning model will produce better results if we help our model generalize better vs making it memorize the data which will render our model useless since we want to be able to predict images it has never seen. . This is where data augmentation comes into play; by applying transformations to our data we can provide a different perspective on the data used by the model which should help us avoid over fitting, since although we might be showing the same image several times we are transforming as we use it, providing the model with access to new data that might help improve the performance of the model. . # and we can see the how the RandomResizeCrop works with an image dls = monsters.dataloaders(path) . dls.train.show_batch(max_n=8, nrows=2, unique=True) . Training the model . We will now create a new model using the data we gathered, and we will perform a data cleanup using the image classifier cleaner utility from FastAI. Something to take note is that I&#39;m using now a larger size for the RandomResizedCrop and also I added the aug_transforms method to the batch transforms which will provide a list of batch transform with default values to execute to the mini batch. . Finally, we will create a new learner with a resnet50 architecture. In the default example, which I&#39;m basing my notebook, Jeremy uses resnet34 but I wanted to try to use a larger network and see how it would work. . monsters = monsters.new( item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = monsters.dataloaders(path) . Fine Tune and Model Head . One important method and concept I learned in the course is fine_tune(). This method when called basically trains the model but only makes modifications to the parameters of the head of a model. . The head of a model is the top layer of a model; meaning that when we fine tune a model we are taking advantage of transferred learning; we are using the values of the previous layers that were trained to recognize some basic images and/or forms that can be useful regardless of the problem we are trying to solve. . An example of the starting layers detections are lines, curves, circles, basic forms, etc. . learn = cnn_learner(dls, resnet50, metrics=error_rate) learn.fine_tune(6) . epoch train_loss valid_loss error_rate time . 0 | 1.996539 | 1.241446 | 0.353211 | 00:15 | . epoch train_loss valid_loss error_rate time . 0 | 0.905812 | 0.716659 | 0.211009 | 00:19 | . 1 | 0.711507 | 0.813167 | 0.192661 | 00:18 | . 2 | 0.576557 | 0.609035 | 0.133028 | 00:19 | . 3 | 0.450786 | 0.601878 | 0.151376 | 00:19 | . 4 | 0.370991 | 0.593832 | 0.137615 | 00:19 | . 5 | 0.309034 | 0.585341 | 0.146789 | 00:18 | . Top Losses Review . After our model has trained we will use the confusion matrix to generate a graph that will help us understand the failures in predictions and later. This method is one of my most favorite one since it really helps people understand what&#39;s going on with our model. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . interp.plot_top_losses(4, nrows=2) . Data Cleaning . As we can see from the top loss review we can try to fix our data and try to enhance our model performance. Something I learned from the course is that the model is going to be as good as your data is, thats why a lot of people spend a lot of time cleaning up data sets and preparing them and also why in most cases (like for this blog post) we spend more time in the next steps: . Review loss and performance | Clean up the data we are using for training | Clean up validation data since it might have labeling issues | Retrain our model with a new cleaner data set | The next lines of code will help us execute our image cleaning process. We will review the images of the data set and use that to retrain the model. . Important . The next cleaning process is done in 2 steps: . Use the cleaning widget and select the correcting action to be performed on the images | Execute the functions below the cleaner since that will be used to actually remove the images | cleaner = ImageClassifierCleaner(learn) cleaner . for idx in cleaner.delete(): print(&#39;Unlinking image: &#39; + str(cleaner.fns[idx])) cleaner.fns[idx].unlink() for idx,cat in cleaner.change(): filename = os.path.dirname(str(cleaner.fns[idx]))+ &#39;/&#39; + cat + &#39;_&#39; + os.path.basename(str(cleaner.fns[idx])) os.rename(str(cleaner.fns[idx]),filename) print(&#39;Moving image to &#39; + filename) shutil.move(filename, path/cat) . Production Inference . The last I want to talk about is that we should prototype as much as we can. We can spend and absurd amount of time trying to gather data, cleaning and training but at some point we need to make sure we release it, put it in production and test it out in the open with real data. . In the case of our Resident Evil model I took a couple of screenshots from RE3 and gave it a go. I passed an image of a Hunter and I obtained decent results. Having said that making sure the application is usable by other people other than us its easier said than done. Fortunately, there are several tools we can use in Jupyter and FastAI to create a small UI, so we can prototype what we have done in a user friendly way. . In the next lines of code we will be exporting our model, then, using the load_learner function, we can load the new model for inference. . learn.export() # This is exporting our DL model into a pkl file our app can use for inference path = Path() path.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . learn_inf = load_learner(path/&#39;export.pkl&#39;) . Widgets, Widgets and More Widgets . In the course they taught us how to use Widgets in Jupyter. Widgets allow us to create a UI that can help us prototype and interact with the DL model. Let&#39;s use the widgets to create a web application for our new Resident Evil Detector. . Something important to mention is that there are several services we can use to host the app and if you are a web developer you can use different frameworks to develop a very cool web app. . Some examples can be found in Chapter 2 of the FastBook. . btn_upload = widgets.FileUpload() btn_upload ### classification btn btn_run = widgets.Button(description=&#39;Classify&#39;) out_pl = widgets.Output() ### Prediction Label lbl_pred = widgets.Label() ### Function to classify images when the btn is pressed def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; ### Attach function to onclick event for the javascript btn btn_run.on_click(on_click_classify) #Putting back btn_upload to a widget for next cell btn_upload = widgets.FileUpload() . VBox([widgets.Label(&#39;Detect RE Monster&#39;), btn_upload, btn_run, out_pl, lbl_pred]) . . Finally we have a new UI and we should be ready to prototype our Resident Evil Detector. I hope you enjoyed the post and also feel free to use and make modifications you can find it on my github and its already a Jupyter Notebook so you can grab the blog post and use it to pilot things out or to add more categories to the detector. . Thanks for taking the time to read my post and have an awesome rest of your day. . References . Fixes for corrupt data EXIF | Resident Evil 3, https://www.residentevil.com/3/ | Fast AI Course V4 Lesson 2, https://course.fast.ai/videos/?lesson=2 | FastAI Documentation aug_transforms | . This blog post is under Fair Use: Copyright Disclaimer Under Section 107 of the Copyright Act in 1976; Allowance is made for &quot;Fair Use&quot; for purposes such as criticism, comment, news reporting, teaching, scholarship, and research. Fair use is a use permitted by copyright statute that might otherwise be infringing. Non-profit, educational or personal use tips the balance in favor of fair use. . All rights and credit go directly to its rightful owners. No copyright infringement intended. . Enjoy. .",
            "url": "https://victor-vargas2009.github.io/gamescode/fastai/resident%20evil/vision/2021/04/18/Resident-Evil-Detector.html",
            "relUrl": "/fastai/resident%20evil/vision/2021/04/18/Resident-Evil-Detector.html",
            "date": " • Apr 18, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://victor-vargas2009.github.io/gamescode/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://victor-vargas2009.github.io/gamescode/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About me...",
          "content": "Hi my name is Victor and you have found the About Me page (you probably ended here by mistake) but wait…don’t go, please continue. . I’m a software engineer who loves to learn new things and also who really likes video games (geeky!!). This is my first time blogging and I’m really excited and hopeful that there is something in this blog that you will find useful, in this blog I will do my best share my experiences with code with all the different things I have learned combined with my geeky passions (video games, movies, series, legos and more). . I think that if we are able to mix things we want to learn with things we really like its easier to learn, at least that’s the way I learn and the way I found easier for me to get develop my skillsets. . Finally Thank you for taking the time to be here and I hope you enjoy the content and you find something useful from the blog (at least some good games, yeah!!). .",
          "url": "https://victor-vargas2009.github.io/gamescode/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://victor-vargas2009.github.io/gamescode/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}